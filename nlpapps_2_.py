# -*- coding: utf-8 -*-
"""NLPapps_2_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QvL0a-agyC88g3NNWv7Xil6INAq9DqwF
"""

!pip install streamlit
!pip install datasets

import torch
import pandas as pd
import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import json
import os
from datasets import Dataset
import numpy as np

# Disable Weights & Biases logging
os.environ["WANDB_DISABLED"] = "true"

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load manually uploaded ChemProt dataset
train_path = "/content/train.jsonl"
test_path = "/content/test.jsonl"

def load_jsonl_data(file_path):
    with open(file_path, "r") as f:
        return [json.loads(line) for line in f]

train_data = load_jsonl_data(train_path)
test_data = load_jsonl_data(test_path)
print("Dataset loaded successfully!")

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("michiyasunaga/BioLinkBERT-base")

# Map labels to numeric values
unique_labels = list(set(d['label'] for d in train_data))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

def preprocess_data(examples):
    inputs = tokenizer([ex['text'] for ex in examples], padding=True, truncation=True, max_length=512)
    labels = [label_to_id[ex['label']] for ex in examples]
    inputs['labels'] = labels
    return inputs

train_encodings = preprocess_data(train_data)
test_encodings = preprocess_data(test_data)

train_dataset = Dataset.from_dict(train_encodings)
test_dataset = Dataset.from_dict(test_encodings)

model = AutoModelForSequenceClassification.from_pretrained("michiyasunaga/BioLinkBERT-base", num_labels=len(unique_labels)).to(device)

# Training arguments
training_args = TrainingArguments(
    output_dir="/content/results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

# Evaluate model
def compute_metrics(pred):
    labels = np.array(pred.label_ids)  # Ensure labels are a NumPy array
    preds = np.argmax(pred.predictions, axis=1)  # Take the highest probability class
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    accuracy = accuracy_score(labels, preds)
    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}

# Get predictions
eval_results = trainer.predict(test_dataset)
eval_metrics = compute_metrics(eval_results)
print("Evaluation Metrics:", eval_metrics)

# Save the trained model
model.save_pretrained("/content/chemprot_relation_model")
tokenizer.save_pretrained("/content/chemprot_relation_model")

# Streamlit Web UI
st.title("Biomedical Relation Extraction")
st.write("Enter biomedical text and extract relations between entities.")

# Input field
text_input = st.text_area("Enter biomedical text:", "")

# Entity Selection
types = ["Gene", "Protein", "Disease", "Drug"]
selected_entity = st.selectbox("Select entity type:", types)

# Extract Relations
if st.button("Extract Relations"):
    if text_input.strip():
        inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True).to(device)
        with torch.no_grad():
            logits = model(**inputs).logits
        predicted_label = torch.argmax(logits, dim=1).item()
        relation = id_to_label[predicted_label]
        st.success(f"Extracted Relation: {relation}")
    else:
        st.error("Please enter some text.")